{
  "best_global_step": 6548,
  "best_metric": 0.3164568245410919,
  "best_model_checkpoint": "./output/sadra_qnli\\checkpoint-6548",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 6548,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015271838729383017,
      "grad_norm": 0.9178212285041809,
      "learning_rate": 0.00019900223986968031,
      "loss": 0.6933,
      "step": 50
    },
    {
      "epoch": 0.030543677458766034,
      "grad_norm": 2.0344347953796387,
      "learning_rate": 0.00019798411728772145,
      "loss": 0.6161,
      "step": 100
    },
    {
      "epoch": 0.04581551618814905,
      "grad_norm": 1.2304631471633911,
      "learning_rate": 0.0001969659947057626,
      "loss": 0.5319,
      "step": 150
    },
    {
      "epoch": 0.06108735491753207,
      "grad_norm": 1.3294588327407837,
      "learning_rate": 0.00019594787212380373,
      "loss": 0.5114,
      "step": 200
    },
    {
      "epoch": 0.07635919364691508,
      "grad_norm": 1.259649634361267,
      "learning_rate": 0.00019492974954184484,
      "loss": 0.5063,
      "step": 250
    },
    {
      "epoch": 0.0916310323762981,
      "grad_norm": 1.1051944494247437,
      "learning_rate": 0.00019391162695988598,
      "loss": 0.4822,
      "step": 300
    },
    {
      "epoch": 0.10690287110568113,
      "grad_norm": 0.9993369579315186,
      "learning_rate": 0.0001928935043779271,
      "loss": 0.4806,
      "step": 350
    },
    {
      "epoch": 0.12217470983506414,
      "grad_norm": 1.3315863609313965,
      "learning_rate": 0.00019187538179596826,
      "loss": 0.4865,
      "step": 400
    },
    {
      "epoch": 0.13744654856444716,
      "grad_norm": 1.401568055152893,
      "learning_rate": 0.00019085725921400937,
      "loss": 0.4674,
      "step": 450
    },
    {
      "epoch": 0.15271838729383017,
      "grad_norm": 2.5356831550598145,
      "learning_rate": 0.0001898391366320505,
      "loss": 0.4468,
      "step": 500
    },
    {
      "epoch": 0.1679902260232132,
      "grad_norm": 1.6878989934921265,
      "learning_rate": 0.00018882101405009164,
      "loss": 0.4478,
      "step": 550
    },
    {
      "epoch": 0.1832620647525962,
      "grad_norm": 1.7168571949005127,
      "learning_rate": 0.00018780289146813278,
      "loss": 0.4429,
      "step": 600
    },
    {
      "epoch": 0.19853390348197922,
      "grad_norm": 3.2201383113861084,
      "learning_rate": 0.00018678476888617392,
      "loss": 0.4589,
      "step": 650
    },
    {
      "epoch": 0.21380574221136225,
      "grad_norm": 2.7032315731048584,
      "learning_rate": 0.00018576664630421503,
      "loss": 0.4632,
      "step": 700
    },
    {
      "epoch": 0.22907758094074526,
      "grad_norm": 1.2162076234817505,
      "learning_rate": 0.00018474852372225617,
      "loss": 0.4396,
      "step": 750
    },
    {
      "epoch": 0.24434941967012827,
      "grad_norm": 1.5145487785339355,
      "learning_rate": 0.00018373040114029728,
      "loss": 0.4463,
      "step": 800
    },
    {
      "epoch": 0.2596212583995113,
      "grad_norm": 2.891125440597534,
      "learning_rate": 0.00018271227855833845,
      "loss": 0.4334,
      "step": 850
    },
    {
      "epoch": 0.2748930971288943,
      "grad_norm": 2.310316801071167,
      "learning_rate": 0.00018169415597637956,
      "loss": 0.4509,
      "step": 900
    },
    {
      "epoch": 0.29016493585827735,
      "grad_norm": 1.3867223262786865,
      "learning_rate": 0.0001806760333944207,
      "loss": 0.4471,
      "step": 950
    },
    {
      "epoch": 0.30543677458766033,
      "grad_norm": 1.6900064945220947,
      "learning_rate": 0.00017965791081246183,
      "loss": 0.4398,
      "step": 1000
    },
    {
      "epoch": 0.32070861331704337,
      "grad_norm": 3.1907811164855957,
      "learning_rate": 0.00017863978823050294,
      "loss": 0.4256,
      "step": 1050
    },
    {
      "epoch": 0.3359804520464264,
      "grad_norm": 1.0232577323913574,
      "learning_rate": 0.0001776216656485441,
      "loss": 0.4378,
      "step": 1100
    },
    {
      "epoch": 0.3512522907758094,
      "grad_norm": 2.1753649711608887,
      "learning_rate": 0.00017660354306658522,
      "loss": 0.4198,
      "step": 1150
    },
    {
      "epoch": 0.3665241295051924,
      "grad_norm": 1.6873699426651,
      "learning_rate": 0.00017558542048462636,
      "loss": 0.4035,
      "step": 1200
    },
    {
      "epoch": 0.38179596823457546,
      "grad_norm": 1.734962821006775,
      "learning_rate": 0.00017456729790266747,
      "loss": 0.4291,
      "step": 1250
    },
    {
      "epoch": 0.39706780696395844,
      "grad_norm": 1.7158058881759644,
      "learning_rate": 0.00017354917532070864,
      "loss": 0.4377,
      "step": 1300
    },
    {
      "epoch": 0.41233964569334147,
      "grad_norm": 1.444449782371521,
      "learning_rate": 0.00017253105273874975,
      "loss": 0.4348,
      "step": 1350
    },
    {
      "epoch": 0.4276114844227245,
      "grad_norm": 2.17288875579834,
      "learning_rate": 0.00017151293015679089,
      "loss": 0.4247,
      "step": 1400
    },
    {
      "epoch": 0.4428833231521075,
      "grad_norm": 2.0537898540496826,
      "learning_rate": 0.00017049480757483202,
      "loss": 0.4455,
      "step": 1450
    },
    {
      "epoch": 0.4581551618814905,
      "grad_norm": 1.969059705734253,
      "learning_rate": 0.00016947668499287313,
      "loss": 0.4213,
      "step": 1500
    },
    {
      "epoch": 0.47342700061087356,
      "grad_norm": 1.4881248474121094,
      "learning_rate": 0.0001684585624109143,
      "loss": 0.4282,
      "step": 1550
    },
    {
      "epoch": 0.48869883934025654,
      "grad_norm": 1.8762707710266113,
      "learning_rate": 0.0001674404398289554,
      "loss": 0.4235,
      "step": 1600
    },
    {
      "epoch": 0.5039706780696396,
      "grad_norm": 1.3152368068695068,
      "learning_rate": 0.00016642231724699655,
      "loss": 0.4167,
      "step": 1650
    },
    {
      "epoch": 0.5192425167990226,
      "grad_norm": 1.4593299627304077,
      "learning_rate": 0.00016540419466503766,
      "loss": 0.4376,
      "step": 1700
    },
    {
      "epoch": 0.5345143555284056,
      "grad_norm": 2.72627329826355,
      "learning_rate": 0.00016438607208307883,
      "loss": 0.4044,
      "step": 1750
    },
    {
      "epoch": 0.5497861942577886,
      "grad_norm": 2.6975197792053223,
      "learning_rate": 0.00016336794950111994,
      "loss": 0.4363,
      "step": 1800
    },
    {
      "epoch": 0.5650580329871716,
      "grad_norm": 1.422565221786499,
      "learning_rate": 0.00016234982691916108,
      "loss": 0.3777,
      "step": 1850
    },
    {
      "epoch": 0.5803298717165547,
      "grad_norm": 2.1792185306549072,
      "learning_rate": 0.00016133170433720221,
      "loss": 0.4238,
      "step": 1900
    },
    {
      "epoch": 0.5956017104459377,
      "grad_norm": 1.3835854530334473,
      "learning_rate": 0.00016031358175524332,
      "loss": 0.4124,
      "step": 1950
    },
    {
      "epoch": 0.6108735491753207,
      "grad_norm": 2.580300807952881,
      "learning_rate": 0.0001592954591732845,
      "loss": 0.4191,
      "step": 2000
    },
    {
      "epoch": 0.6261453879047038,
      "grad_norm": 5.031009674072266,
      "learning_rate": 0.0001582773365913256,
      "loss": 0.4073,
      "step": 2050
    },
    {
      "epoch": 0.6414172266340867,
      "grad_norm": 2.231494426727295,
      "learning_rate": 0.00015725921400936674,
      "loss": 0.4264,
      "step": 2100
    },
    {
      "epoch": 0.6566890653634697,
      "grad_norm": 1.4453444480895996,
      "learning_rate": 0.00015624109142740785,
      "loss": 0.4296,
      "step": 2150
    },
    {
      "epoch": 0.6719609040928528,
      "grad_norm": 3.492774486541748,
      "learning_rate": 0.000155222968845449,
      "loss": 0.4268,
      "step": 2200
    },
    {
      "epoch": 0.6872327428222358,
      "grad_norm": 4.013773441314697,
      "learning_rate": 0.00015420484626349013,
      "loss": 0.3898,
      "step": 2250
    },
    {
      "epoch": 0.7025045815516188,
      "grad_norm": 1.739259123802185,
      "learning_rate": 0.00015318672368153127,
      "loss": 0.4162,
      "step": 2300
    },
    {
      "epoch": 0.7177764202810019,
      "grad_norm": 1.74978506565094,
      "learning_rate": 0.0001521686010995724,
      "loss": 0.421,
      "step": 2350
    },
    {
      "epoch": 0.7330482590103848,
      "grad_norm": 2.8505733013153076,
      "learning_rate": 0.00015115047851761351,
      "loss": 0.3804,
      "step": 2400
    },
    {
      "epoch": 0.7483200977397678,
      "grad_norm": 1.2441720962524414,
      "learning_rate": 0.00015013235593565468,
      "loss": 0.4119,
      "step": 2450
    },
    {
      "epoch": 0.7635919364691509,
      "grad_norm": 1.5879263877868652,
      "learning_rate": 0.0001491142333536958,
      "loss": 0.4104,
      "step": 2500
    },
    {
      "epoch": 0.7788637751985339,
      "grad_norm": 1.4375265836715698,
      "learning_rate": 0.00014809611077173693,
      "loss": 0.4097,
      "step": 2550
    },
    {
      "epoch": 0.7941356139279169,
      "grad_norm": 1.1512938737869263,
      "learning_rate": 0.00014707798818977804,
      "loss": 0.3894,
      "step": 2600
    },
    {
      "epoch": 0.8094074526573,
      "grad_norm": 2.0902302265167236,
      "learning_rate": 0.00014605986560781918,
      "loss": 0.3842,
      "step": 2650
    },
    {
      "epoch": 0.8246792913866829,
      "grad_norm": 1.617728352546692,
      "learning_rate": 0.00014504174302586032,
      "loss": 0.3987,
      "step": 2700
    },
    {
      "epoch": 0.8399511301160659,
      "grad_norm": 3.2106270790100098,
      "learning_rate": 0.00014402362044390146,
      "loss": 0.3836,
      "step": 2750
    },
    {
      "epoch": 0.855222968845449,
      "grad_norm": 1.0714902877807617,
      "learning_rate": 0.0001430054978619426,
      "loss": 0.3905,
      "step": 2800
    },
    {
      "epoch": 0.870494807574832,
      "grad_norm": 2.3384363651275635,
      "learning_rate": 0.0001419873752799837,
      "loss": 0.41,
      "step": 2850
    },
    {
      "epoch": 0.885766646304215,
      "grad_norm": 2.3358657360076904,
      "learning_rate": 0.00014096925269802484,
      "loss": 0.3634,
      "step": 2900
    },
    {
      "epoch": 0.9010384850335981,
      "grad_norm": 1.5790001153945923,
      "learning_rate": 0.00013995113011606598,
      "loss": 0.4015,
      "step": 2950
    },
    {
      "epoch": 0.916310323762981,
      "grad_norm": 1.5820289850234985,
      "learning_rate": 0.00013893300753410712,
      "loss": 0.3965,
      "step": 3000
    },
    {
      "epoch": 0.931582162492364,
      "grad_norm": 3.0644896030426025,
      "learning_rate": 0.00013791488495214823,
      "loss": 0.4225,
      "step": 3050
    },
    {
      "epoch": 0.9468540012217471,
      "grad_norm": 1.7156442403793335,
      "learning_rate": 0.00013689676237018937,
      "loss": 0.394,
      "step": 3100
    },
    {
      "epoch": 0.9621258399511301,
      "grad_norm": 1.7581384181976318,
      "learning_rate": 0.0001358786397882305,
      "loss": 0.3964,
      "step": 3150
    },
    {
      "epoch": 0.9773976786805131,
      "grad_norm": 2.335814952850342,
      "learning_rate": 0.00013486051720627165,
      "loss": 0.391,
      "step": 3200
    },
    {
      "epoch": 0.9926695174098962,
      "grad_norm": 1.1342920064926147,
      "learning_rate": 0.00013384239462431278,
      "loss": 0.4073,
      "step": 3250
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8535603148453231,
      "eval_loss": 0.3440394699573517,
      "eval_runtime": 20.3433,
      "eval_samples_per_second": 268.54,
      "eval_steps_per_second": 33.574,
      "step": 3274
    },
    {
      "epoch": 1.0079413561392792,
      "grad_norm": 1.3718286752700806,
      "learning_rate": 0.0001328242720423539,
      "loss": 0.3631,
      "step": 3300
    },
    {
      "epoch": 1.0232131948686622,
      "grad_norm": 4.253693580627441,
      "learning_rate": 0.00013180614946039503,
      "loss": 0.3595,
      "step": 3350
    },
    {
      "epoch": 1.0384850335980451,
      "grad_norm": 3.1956448554992676,
      "learning_rate": 0.00013078802687843617,
      "loss": 0.3903,
      "step": 3400
    },
    {
      "epoch": 1.0537568723274282,
      "grad_norm": 1.6165868043899536,
      "learning_rate": 0.0001297699042964773,
      "loss": 0.3819,
      "step": 3450
    },
    {
      "epoch": 1.0690287110568113,
      "grad_norm": 4.398917198181152,
      "learning_rate": 0.00012875178171451842,
      "loss": 0.3658,
      "step": 3500
    },
    {
      "epoch": 1.0843005497861942,
      "grad_norm": 1.5456596612930298,
      "learning_rate": 0.00012773365913255956,
      "loss": 0.3446,
      "step": 3550
    },
    {
      "epoch": 1.0995723885155773,
      "grad_norm": 3.074591875076294,
      "learning_rate": 0.0001267155365506007,
      "loss": 0.3672,
      "step": 3600
    },
    {
      "epoch": 1.1148442272449604,
      "grad_norm": 2.1170248985290527,
      "learning_rate": 0.00012569741396864184,
      "loss": 0.3574,
      "step": 3650
    },
    {
      "epoch": 1.1301160659743432,
      "grad_norm": 1.8906729221343994,
      "learning_rate": 0.00012467929138668297,
      "loss": 0.3771,
      "step": 3700
    },
    {
      "epoch": 1.1453879047037263,
      "grad_norm": 2.273897171020508,
      "learning_rate": 0.00012366116880472409,
      "loss": 0.3719,
      "step": 3750
    },
    {
      "epoch": 1.1606597434331094,
      "grad_norm": 1.3592259883880615,
      "learning_rate": 0.00012264304622276522,
      "loss": 0.3637,
      "step": 3800
    },
    {
      "epoch": 1.1759315821624923,
      "grad_norm": 1.7438472509384155,
      "learning_rate": 0.00012162492364080635,
      "loss": 0.3791,
      "step": 3850
    },
    {
      "epoch": 1.1912034208918754,
      "grad_norm": 2.5107719898223877,
      "learning_rate": 0.0001206068010588475,
      "loss": 0.3553,
      "step": 3900
    },
    {
      "epoch": 1.2064752596212585,
      "grad_norm": 1.6119709014892578,
      "learning_rate": 0.00011958867847688861,
      "loss": 0.3645,
      "step": 3950
    },
    {
      "epoch": 1.2217470983506413,
      "grad_norm": 1.740649700164795,
      "learning_rate": 0.00011857055589492976,
      "loss": 0.3544,
      "step": 4000
    },
    {
      "epoch": 1.2370189370800244,
      "grad_norm": 2.137852907180786,
      "learning_rate": 0.00011755243331297087,
      "loss": 0.363,
      "step": 4050
    },
    {
      "epoch": 1.2522907758094075,
      "grad_norm": 2.0528159141540527,
      "learning_rate": 0.00011653431073101203,
      "loss": 0.3516,
      "step": 4100
    },
    {
      "epoch": 1.2675626145387904,
      "grad_norm": 2.7686879634857178,
      "learning_rate": 0.00011551618814905316,
      "loss": 0.3836,
      "step": 4150
    },
    {
      "epoch": 1.2828344532681735,
      "grad_norm": 1.6774117946624756,
      "learning_rate": 0.00011449806556709428,
      "loss": 0.3642,
      "step": 4200
    },
    {
      "epoch": 1.2981062919975566,
      "grad_norm": 2.7710466384887695,
      "learning_rate": 0.00011347994298513543,
      "loss": 0.3499,
      "step": 4250
    },
    {
      "epoch": 1.3133781307269397,
      "grad_norm": 2.264084577560425,
      "learning_rate": 0.00011246182040317654,
      "loss": 0.3484,
      "step": 4300
    },
    {
      "epoch": 1.3286499694563225,
      "grad_norm": 1.5470026731491089,
      "learning_rate": 0.00011144369782121769,
      "loss": 0.3854,
      "step": 4350
    },
    {
      "epoch": 1.3439218081857056,
      "grad_norm": 2.725001811981201,
      "learning_rate": 0.0001104255752392588,
      "loss": 0.378,
      "step": 4400
    },
    {
      "epoch": 1.3591936469150885,
      "grad_norm": 2.2467784881591797,
      "learning_rate": 0.00010940745265729995,
      "loss": 0.3331,
      "step": 4450
    },
    {
      "epoch": 1.3744654856444716,
      "grad_norm": 4.758678436279297,
      "learning_rate": 0.00010838933007534106,
      "loss": 0.3546,
      "step": 4500
    },
    {
      "epoch": 1.3897373243738547,
      "grad_norm": 1.9413713216781616,
      "learning_rate": 0.0001073712074933822,
      "loss": 0.3634,
      "step": 4550
    },
    {
      "epoch": 1.4050091631032378,
      "grad_norm": 1.964564561843872,
      "learning_rate": 0.00010635308491142335,
      "loss": 0.3472,
      "step": 4600
    },
    {
      "epoch": 1.4202810018326206,
      "grad_norm": 2.289654493331909,
      "learning_rate": 0.00010533496232946447,
      "loss": 0.3458,
      "step": 4650
    },
    {
      "epoch": 1.4355528405620037,
      "grad_norm": 3.014907121658325,
      "learning_rate": 0.00010431683974750562,
      "loss": 0.3575,
      "step": 4700
    },
    {
      "epoch": 1.4508246792913866,
      "grad_norm": 2.1083462238311768,
      "learning_rate": 0.00010329871716554673,
      "loss": 0.3658,
      "step": 4750
    },
    {
      "epoch": 1.4660965180207697,
      "grad_norm": 4.311226844787598,
      "learning_rate": 0.00010228059458358788,
      "loss": 0.3129,
      "step": 4800
    },
    {
      "epoch": 1.4813683567501528,
      "grad_norm": 3.680624008178711,
      "learning_rate": 0.00010126247200162899,
      "loss": 0.3609,
      "step": 4850
    },
    {
      "epoch": 1.4966401954795359,
      "grad_norm": 3.658184766769409,
      "learning_rate": 0.00010024434941967013,
      "loss": 0.3423,
      "step": 4900
    },
    {
      "epoch": 1.5119120342089187,
      "grad_norm": 1.6326115131378174,
      "learning_rate": 9.922622683771127e-05,
      "loss": 0.357,
      "step": 4950
    },
    {
      "epoch": 1.5271838729383018,
      "grad_norm": 1.8197439908981323,
      "learning_rate": 9.820810425575239e-05,
      "loss": 0.3692,
      "step": 5000
    },
    {
      "epoch": 1.5424557116676847,
      "grad_norm": 2.183396100997925,
      "learning_rate": 9.718998167379353e-05,
      "loss": 0.3487,
      "step": 5050
    },
    {
      "epoch": 1.5577275503970678,
      "grad_norm": 2.2752912044525146,
      "learning_rate": 9.617185909183466e-05,
      "loss": 0.3541,
      "step": 5100
    },
    {
      "epoch": 1.5729993891264509,
      "grad_norm": 2.349762201309204,
      "learning_rate": 9.51537365098758e-05,
      "loss": 0.3515,
      "step": 5150
    },
    {
      "epoch": 1.588271227855834,
      "grad_norm": 3.177567720413208,
      "learning_rate": 9.413561392791692e-05,
      "loss": 0.3855,
      "step": 5200
    },
    {
      "epoch": 1.6035430665852168,
      "grad_norm": 1.8863033056259155,
      "learning_rate": 9.311749134595806e-05,
      "loss": 0.3689,
      "step": 5250
    },
    {
      "epoch": 1.6188149053146,
      "grad_norm": 4.367888927459717,
      "learning_rate": 9.20993687639992e-05,
      "loss": 0.3301,
      "step": 5300
    },
    {
      "epoch": 1.6340867440439828,
      "grad_norm": 1.9303783178329468,
      "learning_rate": 9.108124618204032e-05,
      "loss": 0.3523,
      "step": 5350
    },
    {
      "epoch": 1.6493585827733659,
      "grad_norm": 2.0190799236297607,
      "learning_rate": 9.006312360008146e-05,
      "loss": 0.3739,
      "step": 5400
    },
    {
      "epoch": 1.664630421502749,
      "grad_norm": 1.7572169303894043,
      "learning_rate": 8.904500101812258e-05,
      "loss": 0.3629,
      "step": 5450
    },
    {
      "epoch": 1.679902260232132,
      "grad_norm": 1.8426405191421509,
      "learning_rate": 8.802687843616372e-05,
      "loss": 0.371,
      "step": 5500
    },
    {
      "epoch": 1.695174098961515,
      "grad_norm": 1.6976877450942993,
      "learning_rate": 8.700875585420485e-05,
      "loss": 0.3554,
      "step": 5550
    },
    {
      "epoch": 1.710445937690898,
      "grad_norm": 1.8001561164855957,
      "learning_rate": 8.599063327224598e-05,
      "loss": 0.3847,
      "step": 5600
    },
    {
      "epoch": 1.725717776420281,
      "grad_norm": 2.4767184257507324,
      "learning_rate": 8.497251069028711e-05,
      "loss": 0.3689,
      "step": 5650
    },
    {
      "epoch": 1.740989615149664,
      "grad_norm": 1.620251178741455,
      "learning_rate": 8.395438810832825e-05,
      "loss": 0.3515,
      "step": 5700
    },
    {
      "epoch": 1.756261453879047,
      "grad_norm": 2.997633695602417,
      "learning_rate": 8.293626552636939e-05,
      "loss": 0.3425,
      "step": 5750
    },
    {
      "epoch": 1.7715332926084302,
      "grad_norm": 1.5781033039093018,
      "learning_rate": 8.191814294441051e-05,
      "loss": 0.3493,
      "step": 5800
    },
    {
      "epoch": 1.786805131337813,
      "grad_norm": 1.6201568841934204,
      "learning_rate": 8.090002036245165e-05,
      "loss": 0.3427,
      "step": 5850
    },
    {
      "epoch": 1.8020769700671961,
      "grad_norm": 1.672825813293457,
      "learning_rate": 7.988189778049277e-05,
      "loss": 0.3573,
      "step": 5900
    },
    {
      "epoch": 1.817348808796579,
      "grad_norm": 2.078263282775879,
      "learning_rate": 7.886377519853391e-05,
      "loss": 0.3477,
      "step": 5950
    },
    {
      "epoch": 1.832620647525962,
      "grad_norm": 1.7999274730682373,
      "learning_rate": 7.784565261657504e-05,
      "loss": 0.3431,
      "step": 6000
    },
    {
      "epoch": 1.8478924862553452,
      "grad_norm": 2.4199235439300537,
      "learning_rate": 7.682753003461617e-05,
      "loss": 0.3447,
      "step": 6050
    },
    {
      "epoch": 1.8631643249847283,
      "grad_norm": 2.265376329421997,
      "learning_rate": 7.58094074526573e-05,
      "loss": 0.3157,
      "step": 6100
    },
    {
      "epoch": 1.8784361637141112,
      "grad_norm": 1.8177410364151,
      "learning_rate": 7.479128487069842e-05,
      "loss": 0.3563,
      "step": 6150
    },
    {
      "epoch": 1.8937080024434942,
      "grad_norm": 2.883423089981079,
      "learning_rate": 7.377316228873958e-05,
      "loss": 0.3524,
      "step": 6200
    },
    {
      "epoch": 1.9089798411728771,
      "grad_norm": 3.6509573459625244,
      "learning_rate": 7.27550397067807e-05,
      "loss": 0.3402,
      "step": 6250
    },
    {
      "epoch": 1.9242516799022602,
      "grad_norm": 1.3852622509002686,
      "learning_rate": 7.173691712482184e-05,
      "loss": 0.3559,
      "step": 6300
    },
    {
      "epoch": 1.9395235186316433,
      "grad_norm": 2.038087844848633,
      "learning_rate": 7.071879454286296e-05,
      "loss": 0.341,
      "step": 6350
    },
    {
      "epoch": 1.9547953573610264,
      "grad_norm": 2.6067638397216797,
      "learning_rate": 6.97006719609041e-05,
      "loss": 0.354,
      "step": 6400
    },
    {
      "epoch": 1.9700671960904093,
      "grad_norm": 2.4514284133911133,
      "learning_rate": 6.868254937894523e-05,
      "loss": 0.3344,
      "step": 6450
    },
    {
      "epoch": 1.9853390348197923,
      "grad_norm": 2.8620615005493164,
      "learning_rate": 6.766442679698635e-05,
      "loss": 0.3397,
      "step": 6500
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8671059857221307,
      "eval_loss": 0.3164568245410919,
      "eval_runtime": 21.4469,
      "eval_samples_per_second": 254.722,
      "eval_steps_per_second": 31.846,
      "step": 6548
    }
  ],
  "logging_steps": 50,
  "max_steps": 9822,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5454990630795216.0,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
